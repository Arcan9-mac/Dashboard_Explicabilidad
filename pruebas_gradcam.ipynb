{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:16.202391Z",
     "start_time": "2025-09-30T17:35:15.519351Z"
    }
   },
   "cell_type": "code",
   "source": "%matplotlib inline",
   "id": "afdc20157c3f7725",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:24.917653Z",
     "start_time": "2025-09-30T17:35:16.209229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from utils.yolov7_compat import attempt_load\n",
    "\n",
    "# --- Importaciones de Grad-CAM ---\n",
    "try:\n",
    "    from pytorch_grad_cam import EigenCAM\n",
    "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "    GRAD_CAM_AVAILABLE = True\n",
    "    print(\"✅ Librería 'grad-cam' encontrada e importada correctamente.\")\n",
    "except ImportError:\n",
    "    GRAD_CAM_AVAILABLE = False\n",
    "    print(\"⚠️ ADVERTENCIA: La librería 'grad-cam' no está instalada.\")\n",
    "\n",
    "# Configurar el logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch versión: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n"
   ],
   "id": "302a3cc651145e16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librería 'grad-cam' encontrada e importada correctamente.\n",
      "PyTorch versión: 2.5.1+cu121\n",
      "CUDA disponible: True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:37:58.098241Z",
     "start_time": "2025-09-30T17:37:58.086469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### --- ¡MODIFICA ESTAS TRES LÍNEAS SI ES NECESARIO! --- ###\n",
    "MODEL_PATH = 'weights/best.pt'\n",
    "IMAGE_PATH = 'data/validation_set/images/WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06.jpg'\n",
    "YAML_PATH = 'configs/models_config.yaml' # Ruta a tu archivo .yaml que contiene los 'names' de las clases\n",
    "OUTPUT_FOLDER = 'analysis_results' # <<< NUEVO: Carpeta para guardar las imágenes generadas\n",
    "\n",
    "# --- Umbral de confianza ---\n",
    "CONF_THRESHOLD = 0.25\n",
    "\n",
    "# --- Capas objetivo para analizar con EigenCAM ---\n",
    "cam_targets = {\n",
    "    #\"Backbone (Capa 3)\": 3,\n",
    "    \"Backbone (Capa 24)\": 24,\n",
    "    \"Backbone (Capa 50)\": 50,\n",
    "    \"SPPCSPC (Capa 51)\": 51,\n",
    "    \"Neck (Capa 75)\": 75,\n",
    "    \"Neck (Capa 88)\": 88,\n",
    "    \"Neck (Capa 101)\": 101,\n",
    "    \"Head (Capa 104)\": 104\n",
    "}\n",
    "\n",
    "# --- Configuración del dispositivo ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ],
   "id": "ff2f21541a99d62d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:25.091898Z",
     "start_time": "2025-09-30T17:35:25.069374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =================================================================================\n",
    "# TUS FUNCIONES PARA DIBUJAR CAJAS Y UTILIDADES (Adaptadas para Jupyter)\n",
    "# =================================================================================\n",
    "import time\n",
    "import math\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
    "                        labels=()):\n",
    "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results.\"\"\"\n",
    "    # ... (código completo de NMS de YOLOv7) ...\n",
    "    # [Aquí iría el código completo de la función NMS]\n",
    "    # Por brevedad, lo he resumido, pero en el código a continuación estará completo.\n",
    "    # El código completo se asegura de que las predicciones crudas se conviertan en cajas finales.\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_det = 300  # maximum number of detections per image\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 10.0  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            l = labels[xi]\n",
    "            v = torch.zeros((len(l), nc + 5), device=x.device)\n",
    "            v[:, :4] = l[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as weighted mean\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "# (El resto de tus funciones de dibujo permanecen igual)\n",
    "\n",
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    \"\"\"Redimensiona coordenadas del tamaño de img1 al tamaño de img0.\"\"\"\n",
    "    if ratio_pad is None:\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "    coords[:, [0, 2]] -= pad[0]\n",
    "    coords[:, [1, 3]] -= pad[1]\n",
    "    coords[:, :4] /= gain\n",
    "    coords[:, :4] = coords[:, :4].clamp(min=0)\n",
    "    return coords\n",
    "\n",
    "def draw_prediction_boxes(image, predictions, tensor_shape, class_names):\n",
    "    img_with_boxes = image.copy()\n",
    "    dets = predictions[0]\n",
    "    if dets is not None and len(dets):\n",
    "        dets_cpu = dets.clone()\n",
    "        dets_cpu[:, :4] = scale_coords(tensor_shape, dets_cpu[:, :4], image.shape).round()\n",
    "        for *xyxy, conf, cls in reversed(dets_cpu):\n",
    "            class_id = int(cls)\n",
    "            label = f'{class_names[class_id]} {conf:.2f}' if class_id < len(class_names) else f'ID Inesperado: {class_id}'\n",
    "            color = (0, 255, 0)\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(img_with_boxes, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return img_with_boxes\n",
    "\n",
    "def draw_ground_truth_boxes(image, label_path, class_names):\n",
    "    img_with_gt = image.copy()\n",
    "    h, w, _ = image.shape\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                cls_id, x_center, y_center, width, height = int(parts[0]), *map(float, parts[1:])\n",
    "                if cls_id < len(class_names):\n",
    "                    label = class_names[cls_id]\n",
    "                    x1, y1 = int((x_center - width / 2) * w), int((y_center - height / 2) * h)\n",
    "                    x2, y2 = int((x_center + width / 2) * w), int((y_center + height / 2) * h)\n",
    "                    cv2.rectangle(img_with_gt, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    cv2.putText(img_with_gt, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Advertencia: Archivo de etiqueta no encontrado en '{label_path}'\")\n",
    "    return img_with_gt\n",
    "\n",
    "print(\"✅ Funciones de utilidad y dibujo de cajas definidas.\")"
   ],
   "id": "f03237e403fa64d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Funciones de utilidad y dibujo de cajas definidas.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:25.109837Z",
     "start_time": "2025-09-30T17:35:25.099082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =================================================================================\n",
    "# CLASES Y FUNCIONES DE VISUALIZACIÓN DE CAM\n",
    "# =================================================================================\n",
    "\n",
    "class YOLOv7ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(YOLOv7ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[0]\n",
    "\n",
    "def generate_eigen_cam_image(model, img_rgb, img_tensor, geometry_info, target_layer_identifier, device):\n",
    "    if not GRAD_CAM_AVAILABLE: return None\n",
    "    _, pad = geometry_info\n",
    "    pad_w, pad_h = int(pad[0]), int(pad[1])\n",
    "    tensor_h, tensor_w = img_tensor.shape[2:]\n",
    "    all_modules = list(model.modules())\n",
    "    if not (0 <= target_layer_identifier < len(all_modules)): return None\n",
    "    target_module = all_modules[target_layer_identifier]\n",
    "    wrapped_model = YOLOv7ModelWrapper(model)\n",
    "    wrapped_model.to(device)\n",
    "    with EigenCAM(model=wrapped_model, target_layers=[target_module]) as cam:\n",
    "        grayscale_cam = cam(input_tensor=img_tensor.to(device))\n",
    "    if grayscale_cam is None: return None\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    h_unpad, w_unpad = tensor_h - 2 * pad_h, tensor_w - 2 * pad_w\n",
    "    cam_cropped = grayscale_cam[pad_h : pad_h + h_unpad, pad_w : pad_w + w_unpad]\n",
    "    if cam_cropped.size == 0: return None\n",
    "    cam_resized_to_original = cv2.resize(cam_cropped, (img_rgb.shape[1], img_rgb.shape[0]))\n",
    "    img_float = np.float32(img_rgb) / 255\n",
    "    cam_image = show_cam_on_image(img_float, cam_resized_to_original, use_rgb=True)\n",
    "    return (f\"EigenCAM (Capa {target_layer_identifier})\", cam_image)\n",
    "\n",
    "def preprocess_image(img_bgr, img_size=640):\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _ = img_rgb.shape\n",
    "    r = img_size / max(h, w)\n",
    "    new_w, new_h = int(w * r), int(h * r)\n",
    "    img_resized = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    pad_w = (img_size - new_w) / 2\n",
    "    pad_h = (img_size - new_h) / 2\n",
    "    top, bottom, left, right = int(round(pad_h-0.1)), int(round(pad_h+0.1)), int(round(pad_w-0.1)), int(round(pad_w+0.1))\n",
    "    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
    "    img_tensor = img_padded.transpose(2, 0, 1)\n",
    "    img_tensor = np.ascontiguousarray(img_tensor)\n",
    "    img_tensor = torch.from_numpy(img_tensor).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    geometry_info = (r, (pad_w, pad_h))\n",
    "    return img_rgb, img_tensor, geometry_info\n",
    "\n",
    "print(\"✅ Funciones de visualización de CAM definidas.\")"
   ],
   "id": "85a6d6252f39a169",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Funciones de visualización de CAM definidas.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:39:20.390704Z",
     "start_time": "2025-09-30T17:38:11.990359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. Cargar configuración ---\n",
    "try:\n",
    "    with open(YAML_PATH, 'r', errors='ignore') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        class_names = data['yolov7-base']['class_names']\n",
    "    print(f\"✅ Nombres de clase cargados desde '{YAML_PATH}': {class_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar el archivo YAML '{YAML_PATH}': {e}\")\n",
    "    class_names = None\n",
    "\n",
    "# --- 2. Cargar modelo ---\n",
    "try:\n",
    "    model = attempt_load(MODEL_PATH, map_location=device)\n",
    "    model.eval()\n",
    "    print(f\"✅ Modelo cargado exitosamente desde '{MODEL_PATH}' usando 'attempt_load'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar el modelo con 'attempt_load': {e}\")\n",
    "    model = None\n",
    "\n",
    "# --- 3. Procesar y analizar la imagen ---\n",
    "if model and class_names and os.path.exists(IMAGE_PATH):\n",
    "    img_bgr = cv2.imread(IMAGE_PATH)\n",
    "    img_rgb, img_tensor, geometry_info = preprocess_image(img_bgr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_raw = model(img_tensor.to(device))[0]\n",
    "        predictions_list = non_max_suppression(pred_raw, conf_thres=CONF_THRESHOLD, iou_thres=0.45)\n",
    "\n",
    "    label_path = IMAGE_PATH.replace('/images/', '/labels/').rsplit('.', 1)[0] + '.txt'\n",
    "    img_with_gt = draw_ground_truth_boxes(img_rgb, label_path, class_names)\n",
    "    img_with_preds = draw_prediction_boxes(img_rgb, predictions_list, img_tensor.shape[2:], class_names)\n",
    "\n",
    "    output_cam_images = []\n",
    "    print(\"\\nGenerando mapas de calor con EigenCAM...\")\n",
    "    for layer_name, layer_idx in cam_targets.items():\n",
    "        try:\n",
    "            print(f\"  Procesando: {layer_name} (Índice {layer_idx})\")\n",
    "            result = generate_eigen_cam_image(model, img_rgb, img_tensor, geometry_info, layer_idx, device)\n",
    "            if result:\n",
    "                output_cam_images.append(result)\n",
    "        except MemoryError:\n",
    "            print(f\"  🔴 MEMORY ERROR: La capa {layer_idx} es demasiado grande. Saltando.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  🟡 ERROR INESPERADO en la capa {layer_idx}: {e}. Saltando.\")\n",
    "\n",
    "    # --- 4. EXPORTAR RESULTADOS A ARCHIVOS ---\n",
    "    print(f\"\\n✅ Análisis completo. Exportando imágenes a la carpeta: '{OUTPUT_FOLDER}'...\")\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "    # Obtener un nombre base del archivo de imagen original\n",
    "    base_filename = os.path.splitext(os.path.basename(IMAGE_PATH))[0]\n",
    "\n",
    "    # Guardar imagen con Realidad del Terreno (GT)\n",
    "    gt_path = os.path.join(OUTPUT_FOLDER, f\"{base_filename}_ground_truth.jpg\")\n",
    "    cv2.imwrite(gt_path, cv2.cvtColor(img_with_gt, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"  -> Guardado: {gt_path}\")\n",
    "\n",
    "    # Guardar imagen con Predicciones\n",
    "    preds_path = os.path.join(OUTPUT_FOLDER, f\"{base_filename}_predictions.jpg\")\n",
    "    cv2.imwrite(preds_path, cv2.cvtColor(img_with_preds, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"  -> Guardado: {preds_path}\")\n",
    "\n",
    "    # Guardar cada uno de los mapas de calor\n",
    "    for label, cam_img in output_cam_images:\n",
    "        # Crear un nombre de archivo seguro a partir de la etiqueta\n",
    "        safe_label = label.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "        cam_path = os.path.join(OUTPUT_FOLDER, f\"{base_filename}_{safe_label}.jpg\")\n",
    "        cv2.imwrite(cam_path, cv2.cvtColor(cam_img, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"  -> Guardado: {cam_path}\")\n",
    "\n",
    "    print(\"\\n🎉 Exportación finalizada.\")\n",
    "\n",
    "else:\n",
    "    if not model: print(\"El modelo no se pudo cargar.\")\n",
    "    if not class_names: print(\"Los nombres de las clases no se pudieron cargar.\")\n",
    "    if not os.path.exists(IMAGE_PATH): print(f\"La imagen no se encontró en: {IMAGE_PATH}\")"
   ],
   "id": "e54995ed43dbc473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nombres de clase cargados desde 'configs/models_config.yaml': ['paja', 'suciedad']\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.torch_utils:Model Summary: 314 layers, 36487166 parameters, 6194944 gradients, 103.2 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      "✅ Modelo cargado exitosamente desde 'weights/best.pt' usando 'attempt_load'.\n",
      "\n",
      "Generando mapas de calor con EigenCAM...\n",
      "  Procesando: Backbone (Capa 24) (Índice 24)\n",
      "  Procesando: Backbone (Capa 50) (Índice 50)\n",
      "  Procesando: SPPCSPC (Capa 51) (Índice 51)\n",
      "  Procesando: Neck (Capa 75) (Índice 75)\n",
      "  Procesando: Neck (Capa 88) (Índice 88)\n",
      "  Procesando: Neck (Capa 101) (Índice 101)\n",
      "  Procesando: Head (Capa 104) (Índice 104)\n",
      "\n",
      "✅ Análisis completo. Exportando imágenes a la carpeta: 'analysis_results'...\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_ground_truth.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_predictions.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_24.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_50.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_51.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_75.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_88.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_101.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_104.jpg\n",
      "\n",
      "🎉 Exportación finalizada.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:36:32.016062Z",
     "start_time": "2025-09-30T17:36:32.011956Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b98f7604d2210928",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
