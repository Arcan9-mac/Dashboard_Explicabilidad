{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:16.202391Z",
     "start_time": "2025-09-30T17:35:15.519351Z"
    }
   },
   "cell_type": "code",
   "source": "%matplotlib inline",
   "id": "afdc20157c3f7725",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:24.917653Z",
     "start_time": "2025-09-30T17:35:16.209229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from utils.yolov7_compat import attempt_load\n",
    "\n",
    "# --- Importaciones de Grad-CAM ---\n",
    "try:\n",
    "    from pytorch_grad_cam import EigenCAM\n",
    "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "    GRAD_CAM_AVAILABLE = True\n",
    "    print(\"‚úÖ Librer√≠a 'grad-cam' encontrada e importada correctamente.\")\n",
    "except ImportError:\n",
    "    GRAD_CAM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è ADVERTENCIA: La librer√≠a 'grad-cam' no est√° instalada.\")\n",
    "\n",
    "# Configurar el logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch versi√≥n: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n"
   ],
   "id": "302a3cc651145e16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠a 'grad-cam' encontrada e importada correctamente.\n",
      "PyTorch versi√≥n: 2.5.1+cu121\n",
      "CUDA disponible: True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:37:58.098241Z",
     "start_time": "2025-09-30T17:37:58.086469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### --- ¬°MODIFICA ESTAS TRES L√çNEAS SI ES NECESARIO! --- ###\n",
    "MODEL_PATH = 'weights/best.pt'\n",
    "IMAGE_PATH = 'data/validation_set/images/WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06.jpg'\n",
    "YAML_PATH = 'configs/models_config.yaml' # Ruta a tu archivo .yaml que contiene los 'names' de las clases\n",
    "OUTPUT_FOLDER = 'analysis_results' # <<< NUEVO: Carpeta para guardar las im√°genes generadas\n",
    "\n",
    "# --- Umbral de confianza ---\n",
    "CONF_THRESHOLD = 0.25\n",
    "\n",
    "# --- Capas objetivo para analizar con EigenCAM ---\n",
    "cam_targets = {\n",
    "    #\"Backbone (Capa 3)\": 3,\n",
    "    \"Backbone (Capa 24)\": 24,\n",
    "    \"Backbone (Capa 50)\": 50,\n",
    "    \"SPPCSPC (Capa 51)\": 51,\n",
    "    \"Neck (Capa 75)\": 75,\n",
    "    \"Neck (Capa 88)\": 88,\n",
    "    \"Neck (Capa 101)\": 101,\n",
    "    \"Head (Capa 104)\": 104\n",
    "}\n",
    "\n",
    "# --- Configuraci√≥n del dispositivo ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ],
   "id": "ff2f21541a99d62d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:25.091898Z",
     "start_time": "2025-09-30T17:35:25.069374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =================================================================================\n",
    "# TUS FUNCIONES PARA DIBUJAR CAJAS Y UTILIDADES (Adaptadas para Jupyter)\n",
    "# =================================================================================\n",
    "import time\n",
    "import math\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
    "                        labels=()):\n",
    "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results.\"\"\"\n",
    "    # ... (c√≥digo completo de NMS de YOLOv7) ...\n",
    "    # [Aqu√≠ ir√≠a el c√≥digo completo de la funci√≥n NMS]\n",
    "    # Por brevedad, lo he resumido, pero en el c√≥digo a continuaci√≥n estar√° completo.\n",
    "    # El c√≥digo completo se asegura de que las predicciones crudas se conviertan en cajas finales.\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_det = 300  # maximum number of detections per image\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 10.0  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            l = labels[xi]\n",
    "            v = torch.zeros((len(l), nc + 5), device=x.device)\n",
    "            v[:, :4] = l[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as weighted mean\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if (time.time() - t) > time_limit:\n",
    "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "# (El resto de tus funciones de dibujo permanecen igual)\n",
    "\n",
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    \"\"\"Redimensiona coordenadas del tama√±o de img1 al tama√±o de img0.\"\"\"\n",
    "    if ratio_pad is None:\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "    coords[:, [0, 2]] -= pad[0]\n",
    "    coords[:, [1, 3]] -= pad[1]\n",
    "    coords[:, :4] /= gain\n",
    "    coords[:, :4] = coords[:, :4].clamp(min=0)\n",
    "    return coords\n",
    "\n",
    "def draw_prediction_boxes(image, predictions, tensor_shape, class_names):\n",
    "    img_with_boxes = image.copy()\n",
    "    dets = predictions[0]\n",
    "    if dets is not None and len(dets):\n",
    "        dets_cpu = dets.clone()\n",
    "        dets_cpu[:, :4] = scale_coords(tensor_shape, dets_cpu[:, :4], image.shape).round()\n",
    "        for *xyxy, conf, cls in reversed(dets_cpu):\n",
    "            class_id = int(cls)\n",
    "            label = f'{class_names[class_id]} {conf:.2f}' if class_id < len(class_names) else f'ID Inesperado: {class_id}'\n",
    "            color = (0, 255, 0)\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(img_with_boxes, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return img_with_boxes\n",
    "\n",
    "def draw_ground_truth_boxes(image, label_path, class_names):\n",
    "    img_with_gt = image.copy()\n",
    "    h, w, _ = image.shape\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                cls_id, x_center, y_center, width, height = int(parts[0]), *map(float, parts[1:])\n",
    "                if cls_id < len(class_names):\n",
    "                    label = class_names[cls_id]\n",
    "                    x1, y1 = int((x_center - width / 2) * w), int((y_center - height / 2) * h)\n",
    "                    x2, y2 = int((x_center + width / 2) * w), int((y_center + height / 2) * h)\n",
    "                    cv2.rectangle(img_with_gt, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    cv2.putText(img_with_gt, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è Advertencia: Archivo de etiqueta no encontrado en '{label_path}'\")\n",
    "    return img_with_gt\n",
    "\n",
    "print(\"‚úÖ Funciones de utilidad y dibujo de cajas definidas.\")"
   ],
   "id": "f03237e403fa64d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones de utilidad y dibujo de cajas definidas.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:35:25.109837Z",
     "start_time": "2025-09-30T17:35:25.099082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =================================================================================\n",
    "# CLASES Y FUNCIONES DE VISUALIZACI√ìN DE CAM\n",
    "# =================================================================================\n",
    "\n",
    "class YOLOv7ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(YOLOv7ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[0]\n",
    "\n",
    "def generate_eigen_cam_image(model, img_rgb, img_tensor, geometry_info, target_layer_identifier, device):\n",
    "    if not GRAD_CAM_AVAILABLE: return None\n",
    "    _, pad = geometry_info\n",
    "    pad_w, pad_h = int(pad[0]), int(pad[1])\n",
    "    tensor_h, tensor_w = img_tensor.shape[2:]\n",
    "    all_modules = list(model.modules())\n",
    "    if not (0 <= target_layer_identifier < len(all_modules)): return None\n",
    "    target_module = all_modules[target_layer_identifier]\n",
    "    wrapped_model = YOLOv7ModelWrapper(model)\n",
    "    wrapped_model.to(device)\n",
    "    with EigenCAM(model=wrapped_model, target_layers=[target_module]) as cam:\n",
    "        grayscale_cam = cam(input_tensor=img_tensor.to(device))\n",
    "    if grayscale_cam is None: return None\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    h_unpad, w_unpad = tensor_h - 2 * pad_h, tensor_w - 2 * pad_w\n",
    "    cam_cropped = grayscale_cam[pad_h : pad_h + h_unpad, pad_w : pad_w + w_unpad]\n",
    "    if cam_cropped.size == 0: return None\n",
    "    cam_resized_to_original = cv2.resize(cam_cropped, (img_rgb.shape[1], img_rgb.shape[0]))\n",
    "    img_float = np.float32(img_rgb) / 255\n",
    "    cam_image = show_cam_on_image(img_float, cam_resized_to_original, use_rgb=True)\n",
    "    return (f\"EigenCAM (Capa {target_layer_identifier})\", cam_image)\n",
    "\n",
    "def preprocess_image(img_bgr, img_size=640):\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _ = img_rgb.shape\n",
    "    r = img_size / max(h, w)\n",
    "    new_w, new_h = int(w * r), int(h * r)\n",
    "    img_resized = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    pad_w = (img_size - new_w) / 2\n",
    "    pad_h = (img_size - new_h) / 2\n",
    "    top, bottom, left, right = int(round(pad_h-0.1)), int(round(pad_h+0.1)), int(round(pad_w-0.1)), int(round(pad_w+0.1))\n",
    "    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
    "    img_tensor = img_padded.transpose(2, 0, 1)\n",
    "    img_tensor = np.ascontiguousarray(img_tensor)\n",
    "    img_tensor = torch.from_numpy(img_tensor).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    geometry_info = (r, (pad_w, pad_h))\n",
    "    return img_rgb, img_tensor, geometry_info\n",
    "\n",
    "print(\"‚úÖ Funciones de visualizaci√≥n de CAM definidas.\")"
   ],
   "id": "85a6d6252f39a169",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones de visualizaci√≥n de CAM definidas.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:39:20.390704Z",
     "start_time": "2025-09-30T17:38:11.990359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. Cargar configuraci√≥n ---\n",
    "try:\n",
    "    with open(YAML_PATH, 'r', errors='ignore') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        class_names = data['yolov7-base']['class_names']\n",
    "    print(f\"‚úÖ Nombres de clase cargados desde '{YAML_PATH}': {class_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar el archivo YAML '{YAML_PATH}': {e}\")\n",
    "    class_names = None\n",
    "\n",
    "# --- 2. Cargar modelo ---\n",
    "try:\n",
    "    model = attempt_load(MODEL_PATH, map_location=device)\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Modelo cargado exitosamente desde '{MODEL_PATH}' usando 'attempt_load'.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar el modelo con 'attempt_load': {e}\")\n",
    "    model = None\n",
    "\n",
    "# --- 3. Procesar y analizar la imagen ---\n",
    "if model and class_names and os.path.exists(IMAGE_PATH):\n",
    "    img_bgr = cv2.imread(IMAGE_PATH)\n",
    "    img_rgb, img_tensor, geometry_info = preprocess_image(img_bgr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_raw = model(img_tensor.to(device))[0]\n",
    "        predictions_list = non_max_suppression(pred_raw, conf_thres=CONF_THRESHOLD, iou_thres=0.45)\n",
    "\n",
    "    label_path = IMAGE_PATH.replace('/images/', '/labels/').rsplit('.', 1)[0] + '.txt'\n",
    "    img_with_gt = draw_ground_truth_boxes(img_rgb, label_path, class_names)\n",
    "    img_with_preds = draw_prediction_boxes(img_rgb, predictions_list, img_tensor.shape[2:], class_names)\n",
    "\n",
    "    output_cam_images = []\n",
    "    print(\"\\nGenerando mapas de calor con EigenCAM...\")\n",
    "    for layer_name, layer_idx in cam_targets.items():\n",
    "        try:\n",
    "            print(f\"  Procesando: {layer_name} (√çndice {layer_idx})\")\n",
    "            result = generate_eigen_cam_image(model, img_rgb, img_tensor, geometry_info, layer_idx, device)\n",
    "            if result:\n",
    "                output_cam_images.append(result)\n",
    "        except MemoryError:\n",
    "            print(f\"  üî¥ MEMORY ERROR: La capa {layer_idx} es demasiado grande. Saltando.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  üü° ERROR INESPERADO en la capa {layer_idx}: {e}. Saltando.\")\n",
    "\n",
    "    # --- 4. EXPORTAR RESULTADOS A ARCHIVOS ---\n",
    "    print(f\"\\n‚úÖ An√°lisis completo. Exportando im√°genes a la carpeta: '{OUTPUT_FOLDER}'...\")\n",
    "\n",
    "    # Crear la carpeta de salida si no existe\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "    # Obtener un nombre base del archivo de imagen original\n",
    "    base_filename = os.path.splitext(os.path.basename(IMAGE_PATH))[0]\n",
    "\n",
    "    # Guardar imagen con Realidad del Terreno (GT)\n",
    "    gt_path = os.path.join(OUTPUT_FOLDER, f\"{base_filename}_ground_truth.jpg\")\n",
    "    cv2.imwrite(gt_path, cv2.cvtColor(img_with_gt, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"  -> Guardado: {gt_path}\")\n",
    "\n",
    "    # Guardar imagen con Predicciones\n",
    "    preds_path = os.path.join(OUTPUT_FOLDER, f\"{base_filename}_predictions.jpg\")\n",
    "    cv2.imwrite(preds_path, cv2.cvtColor(img_with_preds, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"  -> Guardado: {preds_path}\")\n",
    "\n",
    "    # Guardar cada uno de los mapas de calor\n",
    "    for label, cam_img in output_cam_images:\n",
    "        # Crear un nombre de archivo seguro a partir de la etiqueta\n",
    "        safe_label = label.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "        cam_path = os.path.join(OUTPUT_FOLDER, f\"{base_filename}_{safe_label}.jpg\")\n",
    "        cv2.imwrite(cam_path, cv2.cvtColor(cam_img, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"  -> Guardado: {cam_path}\")\n",
    "\n",
    "    print(\"\\nüéâ Exportaci√≥n finalizada.\")\n",
    "\n",
    "else:\n",
    "    if not model: print(\"El modelo no se pudo cargar.\")\n",
    "    if not class_names: print(\"Los nombres de las clases no se pudieron cargar.\")\n",
    "    if not os.path.exists(IMAGE_PATH): print(f\"La imagen no se encontr√≥ en: {IMAGE_PATH}\")"
   ],
   "id": "e54995ed43dbc473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nombres de clase cargados desde 'configs/models_config.yaml': ['paja', 'suciedad']\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.torch_utils:Model Summary: 314 layers, 36487166 parameters, 6194944 gradients, 103.2 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      "‚úÖ Modelo cargado exitosamente desde 'weights/best.pt' usando 'attempt_load'.\n",
      "\n",
      "Generando mapas de calor con EigenCAM...\n",
      "  Procesando: Backbone (Capa 24) (√çndice 24)\n",
      "  Procesando: Backbone (Capa 50) (√çndice 50)\n",
      "  Procesando: SPPCSPC (Capa 51) (√çndice 51)\n",
      "  Procesando: Neck (Capa 75) (√çndice 75)\n",
      "  Procesando: Neck (Capa 88) (√çndice 88)\n",
      "  Procesando: Neck (Capa 101) (√çndice 101)\n",
      "  Procesando: Head (Capa 104) (√çndice 104)\n",
      "\n",
      "‚úÖ An√°lisis completo. Exportando im√°genes a la carpeta: 'analysis_results'...\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_ground_truth.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_predictions.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_24.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_50.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_51.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_75.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_88.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_101.jpg\n",
      "  -> Guardado: analysis_results\\WIN_20250321_15_02_33_Pro_jpg.rf.016b2ed93bc4b1b0387d0faf99879d06_EigenCAM_Capa_104.jpg\n",
      "\n",
      "üéâ Exportaci√≥n finalizada.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:36:32.016062Z",
     "start_time": "2025-09-30T17:36:32.011956Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b98f7604d2210928",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
